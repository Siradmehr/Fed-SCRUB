\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
\usepackage{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2025}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2025}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}
\input{math_commands.tex}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{algpseudocode}
\usepackage{booktabs}  % For \toprule, \midrule, \bottomrule
\usepackage{makecell}
\usepackage{algorithm}
\usepackage{svg}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{enumitem}
\usepackage[toc,page,header]{appendix}
\usepackage{minitoc}
\renewcommand \thepart{}
\renewcommand \partname{}
\algnewcommand{\LeftComment}[1]{\(\triangleright\) {\color{LightCyan}{#1}}}
% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{pifont}
% \usepackage{pmb}
\usepackage{etoc}
\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}
% \usepackage
% \usepackage[sort,round]{natbib}
% \usepackage{tikz}
% \usetikzlibrary{trees, shadows, shapes.geometric}
% for textual citations



% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{%
\newenvironment{rep#1}[1]{%
 \def\rep@title{#2 \ref{##1}}%
 \begin{rep@theorem}}%
 {\end{rep@theorem}}}
\makeatother


\usepackage{xspace}
\usepackage[colorinlistoftodos, color=blue!30!white %   ,shadow
 %,disable
]{todonotes}
\setlength{\marginparwidth}{12ex}
\newcommand{\todoa}[2][]{\todo[size=tiny,color=blue!20!white,#1]{GA: #2}\xspace}
\newcommand{\todol}[2][]{\todo[size=tiny,color=gray!20!white,#1]{TL: #2}\xspace}
\newcommand{\todom}[2][]{\todo[size=tiny,color=gray!20!white,#1]{MK: #2}\xspace}


% \usepackage[colorinlistoftodos, color=blue!30!white %   ,shadow
% 	% ,disable
% ]{todonotes}
% \setlength{\marginparwidth}{2.5cm}
% \newcommand{\todoa}[2][]{\todo[size=tiny,color=blue!20!white,#1]{GA: #2}\xspace}
% \newcommand{\todor}[2][]{\todo[size=tiny,color=red!20!white,#1]{RK: #2}\xspace}


\title{Fed-$f$-SCRUB: Federated Unlearning via SCRUB Based on $f$-divergence}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  David S.~Hippocampus\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}

The growing adoption of Federated Learning (FL) has brought distributed machine learning (ML) to the forefront, enabling collaborative model training while preserving data locality. However, emerging legal and ethical requirements, such as the "right to be forgotten," and the need to counter data poisoning attacks, highlight the critical necessity of efficient data unlearning in FL systems. Unlike traditional ML, where centralized data access facilitates unlearning, FL’s decentralized nature makes the removal of specific data significantly more challenging. In this paper, we propose a novel federated unlearning framework that adapts the strengths of SCRUB based on $f$-divergences to address this gap to federated setup. Extensive experiments validate the effectiveness of our method, showing that it achieves substantial unlearning speed-ups while preserving model performance and offering strong formal guarantees. This work takes an important step toward building federated learning systems that are not only scalable and efficient but also legally compliant, fair, and transparent.

% Deep Machine Unlearning addresses the problem of removing the effect of a subset of data points from a trained model. Machine Unlearning has various implications for the performance of algorithms. A well-known algorithm, SCRUB~\citep{kurmanji2023unboundedmachineunlearning}, has served as a baseline and achieved key objectives such as removing biases, resolving confusion caused by mislabeled data in trained models, and allowing users to exercise their "right to be forgotten" to protect user privacy. Building on this algorithm, we introduce $f$-SCRUB, an extension of SCRUB that employs different $f$-divergences instead of KL divergence. We analyze the role of these divergences and their impact on the resolution of unlearning problems in various scenarios.
\end{abstract}
\section{Introduction}
The rapid progress of modern machine learning (ML) systems is largely driven by the abundance of data available in today’s digital landscape. However, despite the advantages this vast data offers, several critical concerns arise. First, there is the question of whether the individuals or entities contributing data have consented to its use in developing ML models. Second, the integrity of these models can be compromised by the presence of poisoned data and mislabeled examples. Moreover, legal frameworks such as the European Union’s "Right to Be Forgotten", \citep{rosen2011right}, emphasize the increasing importance of prioritizing safety and privacy in the evolving landscape of ML and AI systems.

In response to these concerns, the concept of machine unlearning has emerged~\citep{bourtoule2021machine}. Its primary goal is to remove the influence of specific data points from a trained model. A straightforward method to achieve this, known as exact unlearning, involves retraining the model from scratch without the targeted data. However, this approach is often impractical due to its significant computational cost—especially in the context of large, deep models—making it unsuitable for scenarios where frequent unlearning is required, such as user deletion requests or the detection of malicious data. To overcome this limitation, approximate unlearning techniques have been developed. These aim to adjust the existing model in a way that approximates the outcome of exact unlearning, while significantly reducing the associated computational overhead. By doing so, they strike a balance between privacy compliance and computational efficiency, enabling more scalable and responsive solutions to data removal.



In parallel, privacy concerns—particularly with sensitive data such as health-related data points—have spurred the development of collaborative learning approaches, notably federated learning (FL). FL enables model training across multiple decentralized devices while ensuring that raw data remains local, thereby addressing privacy concerns by minimizing the need to centralize sensitive information. However, while FL improves data privacy during training, it also introduces new challenges in ensuring data removal after training has commenced.

This leads to the emerging field of federated unlearning, which extends the principles of machine unlearning to the federated setting~\cite{romandini2024federated}. Federated unlearning aims to remove the influence of a client's data on the global model without requiring full retraining from scratch. This task is fundamentally more complex than traditional machine unlearning. In centralized ML, the model owner has direct access to both the training data and the model, making unlearning more straightforward. In contrast, Federated Learning (FL) uses a decentralized approach where data remains on clients' devices and only model updates are shared. This means unlearning in FL must function with limited information while respecting the communication and privacy constraints inherent to the federated system.

Moreover, Federated Learning requires a broader approach to unlearning: rather than removing individual data points, it often necessitates eliminating entire clients or groups of updates. This fundamental distinction demands different unlearning frameworks. Furthermore, the collaborative nature of FL means client updates become integrated in the global model, making it challenging to isolate and remove a single participant's effect.

Despite these challenges, federated unlearning is critical for enabling compliance with emerging data protection regulations and for preserving trust in FL systems. Efficient and principled federated unlearning methods are essential not only for meeting legal requirements but also for maintaining the robustness and reliability of FL models in the face of adversarial behaviors or user revocation requests.

\todoa{I think we should discuss what is our difference in comparison with other works.}

Our main contributions are as follows:
\begin{itemize}
    \item We extend the SCRUB framework for federated Unlearning to address ....
    \item Then, we improve the SCRUB performance inspired by $f$-divergences.
    \item Extensive experiments conducted to show performance of our approach.
\end{itemize}
\section{Related Works}
In this section, we discuss notable works in the federated unlearning.
% \newline

\textbf{Federated Unlearning}:
Federated unlearning (FU) can generally be categorized into two distinct scenarios: \textit{active unlearning} and \textit{passive unlearning}. In the active setting, the client requesting data removal actively participates in the unlearning process, assisting the system in mitigating its contribution to the global model. In contrast, passive unlearning assumes that the forgetting client is no longer available or unwilling to cooperate, requiring the remaining clients and the central server to collaboratively eliminate the influence of the departed client’s data. As observed in the majority of federated unlearning literature, the underlying architecture typically involves a number of clients interacting via a central server, which orchestrates communication and model aggregation. Our focus aligns with this centralized federated setup, wherein unlearning mechanisms are implemented across distributed participants under server coordination.\par

As expected, most methods in passive unlearning attempt to reconstruct the model in the absence of the forgetting client. Due to the challenges and limitations associated with this setting, we concentrate our discussion on federated unlearning under the active scenario. Several early works, such as \citep{xiong2023exact} and \citep{xiong2024appro}, addressed unlearning in convex optimization settings. However, given that most modern deep learning systems involve non-convex objectives, the conclusions drawn from these convex approaches are of limited utility in real-world deployments.\par
In the domain of federated unlearning, \citep{jin2024forgettablefederatedlinearlearning} proposed Forgettable Federated Linear Learning with Certified Data Removal , enabling provable data removal in linear models by exploiting their analytical properties. While computationally efficient and privacy-preserving in theory, the method requires sharing gradients and weights during unlearning, which may leak client information.\par

To address unlearning in more complex models,\citep{wang2024goldfishefficientfederatedunlearning} introduced Goldfish, a framework that removes client influence without full retraining. It incorporates a novel loss balancing retained accuracy, removal bias, and confidence, offering an efficient and scalable unlearning solution.\par

\citet{halimi2023federatedunlearningefficientlyerase} proposed Federated Unlearning, where a client locally reverses its contribution by maximizing empirical loss under constraints, followed by limited retraining across remaining clients. The method avoids storing historical updates but may leak information through shared unlearned model updates.\par

\citet{dhasade2024quickdropefficientfederatedunlearning} integrates dataset distillation into the unlearning process, allowing clients to generate compact representations used for gradient ascent unlearning. This reduces computation but risks privacy leakage through shared distilled updates.\par

\citet{10682764}combines confusion-based updates and salience-aware masking to weaken model memory of specific data. By simulating memory degradation and avoiding full retraining, it enables lightweight, instance- to client-level unlearning in federated settings.\par

\section{Preliminaries}



We begin by formalizing the problem of unlearning in the context of Federated Learning (FL). Let $\mathcal{A}$ denote a federated training algorithm such that the resulting global model $\theta \sim \mathcal{A}(S)$ is trained on a distributed dataset $S = \bigcup_{c=1}^{C} S^{(c)}$ across $C$ clients, where $S^{(c)}$ is the local dataset of client $c$. In federated unlearning, each client may request the removal of a subset $S_F^{(c)} \subset S^{(c)}$, resulting in a retained subset $S_R^{(c)} = S^{(c)} \setminus S_F^{(c)}$. The global retained set is then $S_R = \bigcup_{c=1}^{C} S_R^{(c)}$.

Broadly, federated unlearning can be categorized into two main approaches:

\paragraph{Federated Exact Unlearning.}
An unlearning algorithm $\mathcal{U}: \Theta \times 2^{|S|} \to \Theta$ is said to achieve \textit{exact unlearning} if it satisfies the following distributional equivalence:
\[
\mathcal{U}(\mathcal{A}(S), \{S_F^{(c)}\}_{c=1}^C) \overset{d}{=} \mathcal{A}(S_R).
\]
Here, $\overset{d}{=}$ may be interpreted in two ways:
(1) \textbf{Parameter-level equivalence}, where the resulting model parameters are identical or nearly indistinguishable; or
(2) \textbf{Performance-level equivalence}, where the model’s functional behavior is preserved with respect to downstream tasks.
In this work, we adopt the performance-based perspective, prioritizing behavioral similarity over parameter similarity.

\paragraph{Federated Approximate Unlearning:} Due to the high computational cost of retraining from scratch, approximate unlearning methods aim to efficiently remove the influence of $S_F$ without full re-optimization. It’s important to highlight that federated approximate unlearning (FAU) can have multiple interpretations depending on the context of our discussion. For instance, removing the data points of a single user differs significantly from eliminating the effects of poisoned data of a user or multiple users. These distinct objectives suggest that we need tailored metrics to evaluate the effectiveness of FAU. Depending on the underlying motivation, FAU methods generally fall into one of the following scenarios.

\begin{itemize}
\item \textbf{Robustness-Oriented Unlearning:} Designed to mitigate the impact of noisy, poisoned, or otherwise detrimental data, with the aim of improving the model’s generalization.
\item \textbf{Privacy-Oriented Unlearning:} Focuses on eliminating the influence of specific data to comply with privacy regulations such as the GDPR’s ``Right to be Forgotten.'' Here, the goal is for the model to behave \textit{as if} the data had never been used, often evaluated via privacy metrics like membership inference risk.
\end{itemize}

\subsection{Scenario I: \textit{Effect} Unlearning  (Robustness-Oriented)}

This scenario addresses the removal of data influence for reasons such as label noise or data poisoning, without necessitating full retraining.

Let $\theta = \mathcal{A}(S)$ be the global model trained using a federated algorithm such as Federated Averaging (FedAvg), where:
\[
w_t = \sum_{c=1}^C \frac{n_c}{n} w_t^{(c)}, \quad \text{with } n_c = |S^{(c)}|, \quad n = \sum_{c=1}^C n_c.
\]
Each local model $w_t^{(c)}$ is trained on client $c$’s dataset $S^{(c)}$. Suppose each client identifies a forget set $S_F^{(c)}$, such that $S_R^{(c)} = S^{(c)} \setminus S_F^{(c)}$.

the ideal performance of the unlearned model should remain competitive with or even lesser that of the exact unlearning baseline:
\[
\mathcal{R}_R\left(\mathcal{U}\left(\mathcal{A}(S), \{S_F^{(c)}\}_{c=1}^C\right)\right) \leq \mathcal{R}_R\left(\mathcal{A}(S_R)\right)
\]

\subsection{Scenario II: \textit{Data} Unlearning (Data Privacy-Oriented)}

In this scenario, unlearning is motivated by privacy concerns, where clients demand the deletion of specific personal data in accordance with legal or ethical obligations (e.g., GDPR, CCPA). The objective is to ensure that the resulting global model behaves \emph{as if} the forgotten data $S_F = \bigcup_{c=1}^{C} S_F^{(c)}$ had never been used during training.

\paragraph{Privacy Risk:}
Let $\theta = \mathcal{A}(S)$ denote the model trained on the full dataset, and let $\tilde{\theta} = \mathcal{U}(\theta, \{S_F^{(c)}\}_{c=1}^C)$ be the model after unlearning. The goal is to minimize the distinguishability between $\tilde{\theta}$ and $\mathcal{A}(S_R)$, where $S_R = \bigcup_{c=1}^{C} S_R^{(c)}$.

A common metric for evaluating privacy preservation is the risk of \textbf{membership inference attacks} (MIA), where an adversary attempts to determine whether a given data point was part of the training set. For the unlearned model $\tilde{\theta}$, we define the MIA advantage as:
\[
\text{Adv}_{\text{MIA}}(\tilde{\theta}, S_F) = \sup_{x \in S_F} \left| \Pr[\mathcal{A}(S) \text{ trained on } x] - \Pr[\tilde{\theta} \text{ trained on } x] \right|.
\]

\paragraph{Unlearning Objective:} To ensure privacy-compliant unlearning, the algorithm must guarantee:
\begin{equation}
    \text{Adv}_{\text{MIA}}(\tilde{\theta}, S_F) \approx 0,
\end{equation}

while simultaneously maintaining utility on the retained data, ideally they would be equal however the inequality below would hold.
\begin{equation}
 \mathcal{R}_R\left(\mathcal{A}(S_R)\right)  \leq \mathcal{R}_R\left(\mathcal{U}\left(\mathcal{A}(S), \{S_F^{(c)}\}_{c=1}^C\right)\right)
\end{equation}
We would very much prefer that
\begin{equation}
 \mathcal{R}_R\left(\mathcal{A}(S_R)\right)  \approx \mathcal{R}_R\left(\mathcal{U}\left(\mathcal{A}(S), \{S_F^{(c)}\}_{c=1}^C\right)\right)
\end{equation}


\subsection{Illustrative Example: Distinguishing Between Robustness and Privacy-Oriented Unlearning}

To better understand the two application scenarios described above, consider the following example.

Assume a federated learning setup with $C = 5$ clients, each holding $10{,}000$ local training examples. Suppose that two of the clients discover that $20\%$ of their training data is mislabeled due to a data collection error. They notify the server, which initiates an unlearning procedure to remove the influence of the corrupted samples.

In this \textit{robustness-oriented} scenario, the unlearning process is expected to \textit{improve} the generalization performance of the global model, as it eliminates harmful or misleading examples (e.g., noisy or poisoned data). The previously trained model, influenced by this corrupted data, likely exhibited degraded performance. An ideal unlearning algorithm would reverse this effect, resulting in a more accurate and robust model.

Now consider a different case in which the same two clients are legally required to erase $20\%$ of their data due to privacy regulations, such as the GDPR’s “Right to be Forgotten.” Unlike the previous scenario, the data in question is not erroneous but entirely valid and potentially useful for model training. However, continued use of this data could result in legal consequences for both the clients and the service provider.

In this \textit{privacy-oriented} scenario, unlearning is not expected to improve model performance—indeed, removing useful data may degrade it. The primary goal here is to ensure that the resulting model exhibits no detectable influence from the erased data. Specifically, the model must be resistant to any form of Membership Inference Attack (MIA) that could reveal whether a particular datapoint was part of the training process. In legal contexts, such as courtroom investigations, this privacy guarantee serves as proof of compliance.


\section{Methodology}

Let $\mathcal{A}$ denote a centralized federated learning algorithm that outputs a model $\theta \sim \mathcal{A}(S)$ trained on a distributed dataset $S = \bigcup_{c=1}^C S^{(c)}$. Each client $c$ partitions its local data into retained and forget subsets, $S_R^{(c)}$ and $S_F^{(c)}$, respectively. The objective is to obtain an unlearned model $\theta^u$ such that
\[
\theta^u \approx \mathcal{A}(S_R), \quad S_R := \bigcup_{c=1}^C S_R^{(c)}.
\]
Fed-$f$-SCRUB performs $T$ server-coordinated communication rounds, each consisting of a maximization (forgetting) step and a minimization (retention) step. This is followed by $T_{\text{post}}$ additional minimization-only rounds.

\begin{figure}[h]
  \centering
  \includesvg[width=0.8\textwidth]{diag.svg}
  \caption{}
\end{figure}

\subsection{Local Objectives}

Let $d_f(x; w, w_T)$ be an $f$-divergence between model outputs of $w$ and a reference model $w_T$ on input $x$, and let $\ell(h(x; w), y)$ be the prediction loss. For client $c$, we define:
\[
\mathcal{L}_{\mathrm{max}}^{(c)}(w; w_T) = \frac{1}{|S_F^{(c)}|} \sum_{x_f \in S_F^{(c)}} d_f(x_f; w, w_T),
\]
\[
\mathcal{L}_{\mathrm{min}}^{(c)}(w; w_T) = \frac{\alpha}{|S_R^{(c)}|} \sum_{x_r \in S_R^{(c)}} d_f(x_r; w, w_T) + \frac{\gamma}{|S_R^{(c)}|} \sum_{(x_r, y_r)} \ell(h(x_r; w), y_r).
\]
where $d_f(x; p, q)$ defined as:
\[
D_f(p \parallel q) := \sum_{x \in \mathcal{X}} q(x) f\left( \frac{p(x)}{q(x)} \right)
\]
where $f: (0, \infty) \to \mathbb{R}$ is a convex function with the property that $f(1) = 0$ and $P , Q$ are two discrete distribution. This definition implies that $D_f(P \parallel Q) = 0$ if and only if $P = Q$. By choosing different forms for $f$, we obtain different types of divergences. For example, when $f(t) = t \log(t)$, we get the Kullback-Leibler (KL) divergence, which measures the difference between two probability distributions. In this work, we focus on JS-divergence and $\chi^2$-diveregence. The definition of their generator functions are in the table \ref{tab:divergences}.

\begin{table*}[t]
    \centering
    \setlength{\tabcolsep}{10pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.3} % Increase row height
    \caption{Divergences and their corresponding generator functions}
    \label{tab:divergences}
    \resizebox{0.6\textwidth}{!}{\begin{tabular}{>{\centering\arraybackslash}m{4cm} >{\centering\arraybackslash}m{6cm}}
        \toprule
        \textbf{Divergence} & \textbf{Generator Function} $f(t)$ \\
        \midrule
        KL-divergence & $t \log t$ \\
        $\chi^2$-divergence & $(1 - t)^2$ \\
        JS-divergence & $t \log \left( \frac{2t}{1+t} \right) + \log \left( \frac{2}{1+t} \right)$ \\
        \bottomrule
    \end{tabular}}
\end{table*}
\subsection{Federated Updates (Gradient Version)}
At round $t$, the server broadcasts model $w^t$ and instructs clients to sequentially perform:
\[
g_{\mathrm{max}}^{(c)} = \nabla_w \mathcal{L}_{\mathrm{max}}^{(c)}(w_T; w_T), \quad
w_{\mathrm{max}}^{(c)} = w_T + \eta_{\mathrm{max}} \cdot g_{\mathrm{max}}^{(c)}, \quad
w^{t+\frac{1}{2}} = \sum_{c=1}^C \frac{n_f^{(c)}}{n_f} w_{\mathrm{max}}^{(c)},
\]
\[
g_{\mathrm{min}}^{(c)} = \nabla_w \mathcal{L}_{\mathrm{min}}^{(c)}(w^{t+\frac{1}{2}}; w_T), \quad
w_{\mathrm{min}}^{(c)} = w^{t+\frac{1}{2}} - \eta_{\mathrm{min}} \cdot g_{\mathrm{min}}^{(c)}, \quad
w^{t+1} = \sum_{c=1}^C \frac{n_r^{(c)}}{n_r} w_{\mathrm{min}}^{(c)}.
\]

The reference model $w_T$ may be either the original model $w^0$ or the current round model $w^t$. The former is natural for short local updates, while the latter may be preferable for longer local training.

\subsection{Post-Training Minimization (Gradient Version)}

After $T$ unlearning rounds, the model undergoes $T_{\text{post}}$ rounds of minimization-only updates:

\[
g_{\mathrm{min}}^{(c)} = \nabla_w \mathcal{L}_{\mathrm{min}}^{(c)}(w^{T+\tau}; w_T), \quad
w_{\mathrm{min}}^{(c)} = w^{T+\tau} - \eta_{\mathrm{min}} \cdot g_{\mathrm{min}}^{(c)}, \quad
w^{T+\tau+1} = \sum_{c=1}^C \frac{n_c}{n} w_{\mathrm{min}}^{(c)}, \quad \tau = 0, \dots, T_{\text{post}} - 1.
\]

No further maximization is performed in this phase.

\begin{algorithm}[t]
\caption{Fed-$f$-SCRUB: Federated Unlearning via Divergence Optimization (Gradient Version)}
\label{alg:fed-f-scrub-gradient}
\begin{algorithmic}[1]
\Require Initial model $w^0$, number of unlearning rounds $T$, post-training rounds $T_{\mathrm{post}}$, loss weights $\alpha$, $\gamma$, learning rates $\eta_{\mathrm{max}}$, $\eta_{\mathrm{min}}$
\For{$t = 0$ to $T-1$}
    \State Server selects teacher model $w_T \in \{w^0, w^t\}$
    \Comment{Maximization (Forgetting) Phase}
    \ForAll{clients $c \in [C]$ in parallel}
        \State $g_{\mathrm{max}}^{(c)} \gets \nabla_w \mathcal{L}_{\mathrm{max}}^{(c)}(w_T; w_T)$
        \State $w_{\mathrm{max}}^{(c)} \gets w_T + \eta_{\mathrm{max}} \cdot g_{\mathrm{max}}^{(c)}$
    \EndFor
    \State $w^{t+\frac{1}{2}} \gets \sum_{c=1}^C \frac{n_f^{(c)}}{n_f} w_{\mathrm{max}}^{(c)}$

    \Comment{Minimization (Retention) Phase}
    \ForAll{clients $c \in [C]$ in parallel}
        \State $g_{\mathrm{min}}^{(c)} \gets \nabla_w \mathcal{L}_{\mathrm{min}}^{(c)}(w^{t+\frac{1}{2}}; w_T)$
        \State $w_{\mathrm{min}}^{(c)} \gets w^{t+\frac{1}{2}} - \eta_{\mathrm{min}} \cdot g_{\mathrm{min}}^{(c)}$
    \EndFor
    \State $w^{t+1} \gets \sum_{c=1}^C \frac{n_r^{(c)}}{n_r} w_{\mathrm{min}}^{(c)}$
\EndFor

\For{$\tau = 0$ to $T_{\mathrm{post}} - 1$}
    \State Server selects $w_T \in \{w^0, w^{T+\tau}\}$
    \ForAll{clients $c \in [C]$ in parallel}
        \State $g_{\mathrm{min}}^{(c)} \gets \nabla_w \mathcal{L}_{\mathrm{min}}^{(c)}(w^{T+\tau}; w_T)$
        \State $w_{\mathrm{min}}^{(c)} \gets w^{T+\tau} - \eta_{\mathrm{min}} \cdot g_{\mathrm{min}}^{(c)}$
    \EndFor
    \State $w^{T+\tau+1} \gets \sum_{c=1}^C \frac{n_c}{n} w_{\mathrm{min}}^{(c)}$
\EndFor
\State \Return Final unlearned model $w^{T + T_{\mathrm{post}}}$
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[h]
% \caption{,,,,}
% \label{alg:kl-rlhf}
% \begin{algorithmic}[1]
% \Require ...
% \For{$i = 1,\ldots,m$}
%     \State ...
% \EndFor
% \State ...
% \State ...

% \end{algorithmic}
% \end{algorithm}


\clearpage
\newpage
\bibliography{Refs}
\bibliographystyle{plainnat}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix

\section{Empirical Results}
\subsection{Baselines}
Based on the two main scenarios addressed in our federated unlearning (FU) framework, we select two corresponding baselines for comparison.
First, we consider the approach in \cite{halimi2023federatedunlearningefficientlyerase}, which primarily focuses on privacy by enabling the removal of a client’s entire dataset. In contrast, our framework offers a finer granularity by allowing partial data removal from a client.
Second, for evaluating the removal of poisoned data, we adopt the method proposed in \cite{wang2024goldfishefficientfederatedunlearning} as our baseline, where the focus is on mitigating the effects of backdoor attack datapoints.
\subsection{Simulation details}
\subsection{Evaluation Metrics}
For evaluating privacy, we employ Membership Inference Attacks (MIAs), including well-known variants such as Shokri’s attack \cite{shokri2017membershipinferenceattacksmachine} and Yeom’s attack\cite{yeom2018privacyriskmachinelearning}.
In the context of poisoned or noisy data, following the literature, we use classification accuracy and backdoor attack success rate as the primary evaluation metrics.
\section{More Related Works}
\todoa{combine these two paragraphs}
\textbf{Machine Unlearning}:
As previously mentioned, a foundational mathematical framework for machine unlearning has been developed using principles inspired by differential privacy. While this framework has led to significant progress, its success has largely been restricted to convex optimization problems. Unfortunately, such formulations are not directly applicable to modern deep learning models, which typically involve non-convex objectives and are prone to model memorization—a phenomenon where models retain specific training data rather than learning generalizable patterns. This memorization poses a substantial challenge to unlearning, as sensitive or malicious data embedded in model parameters may persist even after standard removal techniques. Moreover, recent research has shown that it is possible to obtain arbitrarily similar model weights when training on two non-overlapping datasets. This observation implies that reaching a particular point in parameter space does not guarantee effective unlearning, since memorized data influences may not be fully erased. In addition to these theoretical insights, there has been considerable progress in developing unlearning algorithms for classical machine learning models, further illustrating the diverse and evolving landscape of this field.\par

A growing body of research has explored data-driven approaches to machine unlearning, aiming to efficiently remove the influence of specific data points. One recent work, [X]\todoa{???}, employs data attribution techniques to identify and eliminate the effects of targeted data points; however, this approach risks leaking the data of "forgetting" clients to others, making it less suitable for federated unlearning scenarios, which we do not emphasize here. Another notable method, SCRUB, has been developed to enhance stability during the unlearning process, offering a more robust alternative. Parallel to these efforts, a distinct line of work focuses on sparsity-regularized fine-tuning and partial fine-tuning, leveraging model sparsity to reduce computational overhead while unlearning. Additionally, several studies have adopted Bayesian and variational inference techniques to estimate the impact of forgetting data points, providing probabilistic frameworks for unlearning. These diverse approaches underscore the multifaceted nature of machine unlearning, balancing efficiency, privacy, and model integrity across different contexts.\par

\textbf{Machine Unlearning:} Two primary frameworks have emerged to address the challenge of unlearning: exact unlearning \citep{cao2015towards} and approximate unlearning \citep{nguyen2020variational}. Exact unlearning requires retraining the model from scratch using only the remaining data, but this approach is computationally expensive and impractical for large-scale models \citep{thudi2022unrolling}. In contrast, approximate unlearning modifies the trained model to mimic the outcome of retraining on the remaining dataset. The key challenge in approximate unlearning is to ensure that the modified model is indistinguishable from a retrained one, often necessitating theoretical guarantees on the quality of the approximation \citep{guo2019certified}.

Although much of the unlearning research has focused on convex models \citep{sekhari2021rememberwantforgetalgorithms}, the non-convexity of deep neural networks complicates the process. As a result, effective unlearning remains a challenge, with heuristics often producing varying results across different benchmarks, making it difficult to ensure consistent reliability \citep{li2024machine}.

\citep{hayes2024} highlight a significant challenge in fine-tuning-based unlearning methods, known as the \textit{missing targets} problem. When unlearning a data point $x \in \textit{forget set}$, these methods typically apply gradient ascent on $x$ and gradient descent on the retain set to preserve model performance. However, gradient ascent can cause the loss on $x$ to grow indefinitely if unchecked. The desired outcome is to stop when the model's loss on $x$ matches the counterfactual loss (i.e., the loss of a model trained only on the retain set). This presents two main issues: (a) the target loss is unknown, and (b) the optimal stopping point may vary for different points in the forget set. As a result, unlearning algorithms often "undershoot" or "overshoot" the target loss \citep{hayes2024}.

This problem is further analyzed in the work of \citep{georgiev2024attributetodeletemachineunlearningdatamodel}, which uses data modeling to address these challenges. Our research seeks to extend SCRUB to overcome this issue by introducing a loss function that is naturally robust to overshooting and undershooting by employing various $f$-divergences.

While $f$-divergences have been effective loss functions in various machine learning tasks ~\citep{aminian2024robustsemisupervisedlearningfdivergence,roulet2025lossfunctionsoperatorsgenerated,novello2024fdivergencebasedclassificationuse,wang2023reverseklgeneralizingdirect}, they have been primarily used for validating machine unlearning processes. For example, Jensen-Shannon (JS) divergence has been applied in the context of unlearning to validate the removal of data from models \citep{bonato2025retain}, \citep{jeon2024information}, \citep{rangel2024learning}. Furthermore, there has been some exploration of using $f$-divergences specifically for unlearning large language models (LLMs) \citep{wang2024llm}.
\section{Motivations for JS divergence and \texorpdfstring{$\chi^2$}{chi-square}-divergence}
\label{appendix:A}
In this section, we study some motivations behind choosing JS divergence and $\chi^2$ divergence. These information measures offer several advantages over KL divergence, particularly in applications involving generative modeling and robust regularization.

JS divergence is widely used as a loss function in Generative Adversarial Networks (GANs) due to its symmetric and bounded nature, which provides a stable measure of similarity between distributions \citep{goodfellow2014generativeadversarialnetworks}. Unlike KL divergence, which can diverge to infinity when the two distributions have disjoint supports, JS divergence remains finite and well-behaved, making it particularly effective for comparing empirical distributions (\citep{nowozin2016fgantraininggenerativeneural}). This property is especially beneficial in our context, as it helps mitigate overshoot and undershoot problems, particularly in scenarios where exact loss values for removed data points are unavailable.

On the other hand, $\chi^2$ divergence emphasizes large discrepancies due to its squared difference term, making it particularly useful in outlier detection and robust learning frameworks \citep{reid2009informationdivergenceriskbinary}. Regularizing with $\chi^2$ divergence can also help prevent models from becoming overly biased toward majority classes by strongly penalizing large probability gaps \citep{duchi2020learningmodelsuniformperformance}. This property makes it particularly effective in imbalanced learning scenarios, where standard loss functions may fail to capture significant disparities between class distributions.

Thus, by leveraging JS divergence for stable probability comparisons and $\chi^2$ divergence for strong regularization and outlier sensitivity, we can achieve a more robust and balanced learning framework compared to using KL divergence alone.

Building on this, we modify our loss functions and introduce $f$-SCRUB, where we select different $f$-divergences for the retain set and the forget set. Each divergence term, $d(x_r; w^u)$ and $d(x_f; w^u)$, can be chosen from JS, KL, or $\chi^2$ divergences (\citep{Nguyen_2010}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\newpage
\section*{NeurIPS Paper Checklist}

%%% BEGIN INSTRUCTIONS %%%
The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: {\bf The papers not including the checklist will be desk rejected.} The checklist should follow the references and follow the (optional) supplemental material.  The checklist does NOT count towards the page
limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:
\begin{itemize}
    \item You should answer \answerYes{}, \answerNo{}, or \answerNA{}.
    \item \answerNA{} means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
    \item Please provide a short (1–2 sentence) justification right after your answer (even for NA).
   % \item {\bf The papers not including the checklist will be desk rejected.}
\end{itemize}

{\bf The checklist answers are an integral part of your paper submission.} They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "\answerYes{}" is generally preferable to "\answerNo{}", it is perfectly acceptable to answer "\answerNo{}" provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "\answerNo{}" or "\answerNA{}" is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer \answerYes{} to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:
\begin{itemize}
    \item {\bf Delete this instruction block, but keep the section heading ``NeurIPS Paper Checklist"},
    \item  {\bf Keep the checklist subsection headings, questions/answers and guidelines below.}
    \item {\bf Do not modify the questions and only use the provided macros for your answers}.
\end{itemize}


%%% END INSTRUCTIONS %%%


\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the abstract and introduction do not include the claims made in the paper.
        \item The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
        \item The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
        \item It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
    \end{itemize}

\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
        \item The authors are encouraged to create a separate "Limitations" section in their paper.
        \item The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
        \item The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
        \item The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
        \item The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
        \item If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
        \item While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
    \end{itemize}

\item {\bf Theory assumptions and proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include theoretical results.
        \item All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
        \item All assumptions should be clearly stated or referenced in the statement of any theorems.
        \item The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
        \item Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
        \item Theorems and Lemmas that the proof relies upon should be properly referenced.
    \end{itemize}

    \item {\bf Experimental result reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
        \item If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
        \item Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
        \item While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example
        \begin{enumerate}
            \item If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.
            \item If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.
            \item If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
            \item We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
        \end{enumerate}
    \end{itemize}


\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that paper does not include experiments requiring code.
        \item Please see the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item While we encourage the release of code and data, we understand that this might not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
        \item The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (\url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
        \item The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
        \item The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
        \item At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
        \item Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
    \end{itemize}


\item {\bf Experimental setting/details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
        \item The full details can be provided either with the code, in appendix, or as supplemental material.
    \end{itemize}

\item {\bf Experiment statistical significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
        \item The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
        \item The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
        \item The assumptions made should be given (e.g., Normally distributed errors).
        \item It should be clear whether the error bar is the standard deviation or the standard error of the mean.
        \item It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96\% CI, if the hypothesis of Normality of errors is not verified.
        \item For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
        \item If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
    \end{itemize}

\item {\bf Experiments compute resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not include experiments.
        \item The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
        \item The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
        \item The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
    \end{itemize}

\item {\bf Code of ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics \url{https://neurips.cc/public/EthicsGuidelines}?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
        \item If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
        \item The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
    \end{itemize}


\item {\bf Broader impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that there is no societal impact of the work performed.
        \item If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
        \item Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
        \item The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
        \item The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
        \item If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
    \end{itemize}

\item {\bf Safeguards}
    \item[] Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper poses no such risks.
        \item Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
        \item Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
        \item We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
    \end{itemize}

\item {\bf Licenses for existing assets}
    \item[] Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not use existing assets.
        \item The authors should cite the original paper that produced the code package or dataset.
        \item The authors should state which version of the asset is used and, if possible, include a URL.
        \item The name of the license (e.g., CC-BY 4.0) should be included for each asset.
        \item For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
        \item If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, \url{paperswithcode.com/datasets} has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
        \item For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
        \item If this information is not available online, the authors are encouraged to reach out to the asset's creators.
    \end{itemize}

\item {\bf New assets}
    \item[] Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not release new assets.
        \item Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
        \item The paper should discuss whether and how consent was obtained from people whose asset is used.
        \item At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
    \end{itemize}

\item {\bf Crowdsourcing and research with human subjects}
    \item[] Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
        \item According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
    \end{itemize}

\item {\bf Institutional review board (IRB) approvals or equivalent for research with human subjects}
    \item[] Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
        \item Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
        \item We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
        \item For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
    \end{itemize}

\item {\bf Declaration of LLM usage}
    \item[] Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required.
    %this research?
    \item[] Answer: \answerTODO{} % Replace by \answerYes{}, \answerNo{}, or \answerNA{}.
    \item[] Justification: \justificationTODO{}
    \item[] Guidelines:
    \begin{itemize}
        \item The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components.
        \item Please refer to our LLM policy (\url{https://neurips.cc/Conferences/2025/LLM}) for what should or should not be described.
    \end{itemize}

\end{enumerate}







\end{document}