{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-30T21:06:26.382909Z",
     "start_time": "2025-03-30T21:06:26.227460Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from ast import literal_eval\n",
    "from dotenv import dotenv_values\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "\n",
    "# Constants\n",
    "INT_KEYS = [\n",
    "    \"RETRAIN_BATCH\", \"FORGET_BATCH\", \"VAL_BATCH\", \"TEST_BATCH\",\n",
    "    \"NUM_CLASSES\", \"LOCAL_EPOCHS\", \"MIN_EPOCHS\", \"MAX_EPOCHS\", \"LAST_MAX_STEPS\"\n",
    "]\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device(config: Dict) -> torch.device:\n",
    "    \"\"\"Get the appropriate device based on configuration and availability.\"\"\"\n",
    "    device_name = config.get(\"DEVICE\", \"cuda:0\") if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device_name)\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.set_device(device)\n",
    "    return device\n",
    "\n",
    "\n",
    "def load_config(path: str = \"./envs\") -> Dict:\n",
    "    \"\"\"Load and process configuration from environment files.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Configuration directory not found: {path}\")\n",
    "\n",
    "    # Load configuration files\n",
    "    env_path = os.path.join(path, \".env\")\n",
    "    training_path = os.path.join(path, \".env.training\")\n",
    "\n",
    "    if not os.path.exists(env_path) or not os.path.exists(training_path):\n",
    "        raise FileNotFoundError(f\"Required configuration files missing in {path}\")\n",
    "\n",
    "    config = {\n",
    "        **dotenv_values(env_path),\n",
    "        **dotenv_values(training_path),\n",
    "    }\n",
    "\n",
    "    # Process configuration values\n",
    "    try:\n",
    "        # Handle forget class\n",
    "        if \"FORGET_CLASS\" in config:\n",
    "            config[\"FORGET_CLASS\"] = literal_eval(config[\"FORGET_CLASS\"])\n",
    "\n",
    "        # Convert integer keys\n",
    "        for key in INT_KEYS:\n",
    "            if key in config:\n",
    "                config[key] = int(config[key])\n",
    "\n",
    "        # Process comma-separated integer lists\n",
    "        for key in [\"CLIENT_ID_TO_FORGET\", \"LR_ROUND\"]:\n",
    "            if config[key]:\n",
    "                config[key] = [int(i) for i in str(config[key]).split(\",\")]\n",
    "            else:\n",
    "                config[key] = []\n",
    "\n",
    "    except (ValueError, SyntaxError) as e:\n",
    "        raise ValueError(f\"Error parsing configuration: {e}\")\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def np_index_save(full_training_index, training_set, retrain_index, forget_index, val_index, test_index, config, partition_id) -> None:\n",
    "    \"\"\"Save dataset partition indexes as numpy arrays in an npz file.\n",
    "\n",
    "    Args:\n",
    "        full_training_index: Complete training dataset indexes\n",
    "        forget_index: Indexes of samples to be forgotten\n",
    "        val_index: Validation dataset indexes\n",
    "        test_index: Test dataset indexes\n",
    "        retrain_index: Retrain dataset indexes\n",
    "        config: Configuration dictionary containing saving directory\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    save_path = os.path.join(config[\"SAVING_DIR\"], \"partition_indexes\")\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Create the full path for the npz file\n",
    "    file_path = os.path.join(save_path, f\"{partition_id}_dataset_partitions.npz\")\n",
    "\n",
    "    # Save all indexes in a single npz file\n",
    "    np.savez(\n",
    "        file_path,\n",
    "        training_set=training_set,\n",
    "        full_training=full_training_index,\n",
    "        forget=forget_index,\n",
    "        val=val_index,\n",
    "        test=test_index,\n",
    "        retrain=retrain_index\n",
    "    )\n",
    "\n",
    "    print(f\"Dataset partition indexes saved to {file_path}\")\n",
    "\n",
    "def np_index_load(config, partition_id=None) -> tuple:\n",
    "        \"\"\"Load dataset partition indexes from an npz file.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration dictionary containing saving directory\n",
    "            partition_id: Optional partition ID. If None, loads without partition ID in filename\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing (dictionary of indexes, full_training_index, forget_index,\n",
    "                             val_index, test_index, retrain_index)\n",
    "        \"\"\"\n",
    "        # Construct the path to the partition_indexes directory\n",
    "        save_path = os.path.join(config[\"SAVING_DIR\"], \"partition_indexes\")\n",
    "\n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(save_path):\n",
    "            raise FileNotFoundError(f\"Directory not found: {save_path}\")\n",
    "\n",
    "        # Construct the filename based on whether partition_id is provided\n",
    "        if partition_id is not None:\n",
    "            file_path = os.path.join(save_path, f\"{partition_id}_dataset_partitions.npz\")\n",
    "        else:\n",
    "            file_path = os.path.join(save_path, \"dataset_partitions.npz\")\n",
    "\n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        # Load the npz file\n",
    "        loaded_data = np.load(file_path)\n",
    "\n",
    "        # Create the dictionary\n",
    "        indexes_dict = {\n",
    "            \"training_set\": loaded_data[\"training_set\"],\n",
    "            \"full_training_index\": loaded_data[\"full_training\"],\n",
    "            \"forget_index\": loaded_data[\"forget\"],\n",
    "            \"val_index\": loaded_data[\"val\"],\n",
    "            \"test_index\": loaded_data[\"test\"],\n",
    "            \"retrain_index\": loaded_data[\"retrain\"]\n",
    "        }\n",
    "\n",
    "        print(f\"Dataset partition indexes loaded from {file_path}\")\n",
    "\n",
    "        # Return both dictionary and individual arrays\n",
    "        return indexes_dict\n",
    "\n",
    "\n",
    "def setup_experiment(path: str = \"./envs\", load_model_flag = True) -> Dict:\n",
    "    \"\"\"Set up the experiment with configuration, directories, and model.\"\"\"\n",
    "    # Load configuration\n",
    "    config = load_config(path)\n",
    "\n",
    "    # Create saving directory\n",
    "    saving_directory = os.path.join(\n",
    "        \"./checkpoints\",\n",
    "        config[\"CONFIG_ID\"],\n",
    "        config[\"MODEL\"],\n",
    "        config[\"DATASET\"],\n",
    "        f\"{config['CONFIG_NUMBER']}_{config['SEED']}\"\n",
    "    )\n",
    "    os.makedirs(saving_directory, exist_ok=True)\n",
    "    config[\"SAVING_DIR\"] = saving_directory\n",
    "\n",
    "    # Save configuration\n",
    "    config_path = os.path.join(saving_directory, \"custom_config.json\")\n",
    "    config[\"CUSTOM_CONFIG_PATH\"] = saving_directory\n",
    "    if not load_model_flag:\n",
    "        return config\n",
    "    with open(config_path, \"w\") as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "    # Load initial model\n",
    "    config[\"LOADED_MODEL\"] = load_model(config[\"MODEL\"], config.get(\"RESUME\", \"\"))\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "def load_model(model_name: str, checkpoint_path: Optional[str] = None) -> torch.nn.Module:\n",
    "    \"\"\"Load a model and initialize from checkpoint if provided.\"\"\"\n",
    "    model = get_model(model_name)\n",
    "    print(f\"Model '{model_name}' initialized\")\n",
    "\n",
    "    if not checkpoint_path or checkpoint_path in (\"None\", \"\"):\n",
    "        print(\"Using freshly initialized model (no checkpoint loaded)\")\n",
    "        return model\n",
    "\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        print(f\"Warning: No checkpoint found at {checkpoint_path}\")\n",
    "        return model\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "        # Handle different checkpoint formats\n",
    "        if \"state_dict\" in checkpoint:\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            print(\"Successfully loaded model state_dict\")\n",
    "        elif \"model\" in checkpoint:\n",
    "            model.load_state_dict(checkpoint[\"model\"])\n",
    "            print(\"Successfully loaded model weights\")\n",
    "        else:\n",
    "            # Try loading directly\n",
    "            model.load_state_dict(checkpoint)\n",
    "            print(\"Successfully loaded model weights directly\")\n",
    "\n",
    "        print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {str(e)}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_model(\n",
    "        model: torch.nn.Module,\n",
    "        config: Dict,\n",
    "        round: Optional[int] = None,\n",
    "        is_best: bool = False\n",
    ") -> str:\n",
    "    \"\"\"Save model checkpoint to specified path.\"\"\"\n",
    "    # Create save directory\n",
    "    save_dir = os.path.join(config[\"SAVING_DIR\"], \"models_chkpts\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Determine filename\n",
    "    if is_best:\n",
    "        filename = os.path.join(save_dir, \"model_best.pth\")\n",
    "    elif round is not None:\n",
    "        #filename = os.path.join(save_dir, f\"model_round_{round}.pth\")\n",
    "        filename = os.path.join(save_dir, \"model_latest.pth\")\n",
    "    else:\n",
    "        filename = os.path.join(save_dir, \"model_latest.pth\")\n",
    "\n",
    "    # Prepare and save checkpoint\n",
    "    checkpoint = {\"state_dict\": model.state_dict()}\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "    return filename"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:06:28.640272Z",
     "start_time": "2025-03-30T21:06:28.362208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "indexes_list = []\n",
    "for i in [0,1,2,3,4]:\n",
    "    indexes_list.append(np_index_load(setup_experiment(\"./envs\", False), i))"
   ],
   "id": "c271058901a7364d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset partition indexes loaded from ./checkpoints/2/ResNet18_small_test/cifar10/2_17/partition_indexes/0_dataset_partitions.npz\n",
      "Dataset partition indexes loaded from ./checkpoints/2/ResNet18_small_test/cifar10/2_17/partition_indexes/1_dataset_partitions.npz\n",
      "Dataset partition indexes loaded from ./checkpoints/2/ResNet18_small_test/cifar10/2_17/partition_indexes/2_dataset_partitions.npz\n",
      "Dataset partition indexes loaded from ./checkpoints/2/ResNet18_small_test/cifar10/2_17/partition_indexes/3_dataset_partitions.npz\n",
      "Dataset partition indexes loaded from ./checkpoints/2/ResNet18_small_test/cifar10/2_17/partition_indexes/4_dataset_partitions.npz\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:12:23.389861Z",
     "start_time": "2025-03-30T21:12:23.327959Z"
    }
   },
   "cell_type": "code",
   "source": "indexes_list[0][\"test_index\"]",
   "id": "d648690454299b3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 314, 1798, 1016, ..., 1813,  268, 1542], shape=(2000,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:12:14.044994Z",
     "start_time": "2025-03-30T21:12:13.967179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "indexes_list[0][\"full_training_index\"]"
   ],
   "id": "2328b869b2be49ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 376, 7110, 4053, ...,  959, 4192, 4228], shape=(10000,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:13:03.011877Z",
     "start_time": "2025-03-30T21:13:02.954088Z"
    }
   },
   "cell_type": "code",
   "source": "indexes_list[0].keys()",
   "id": "f46cbc7e363eae4e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training_set', 'full_training_index', 'forget_index', 'val_index', 'test_index', 'retrain_index'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:14:24.493054Z",
     "start_time": "2025-03-30T21:14:24.440445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for j in [0,1,2,3,4]:\n",
    "    print(j)\n",
    "    indexes = indexes_list[j]\n",
    "    whole = set()\n",
    "    print(\"len whole=\", len(whole))\n",
    "    for i in [indexes[\"val_index\"], indexes[\"training_set\"]]:\n",
    "        print(len(i))\n",
    "        whole = whole.union(set(i))\n",
    "        print(\"len whole=\", len(whole))\n",
    "\n",
    "    print(len(whole))\n"
   ],
   "id": "accaa1bba3aeaae3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "len whole= 0\n",
      "1000\n",
      "len whole= 1000\n",
      "9000\n",
      "len whole= 10000\n",
      "10000\n",
      "1\n",
      "len whole= 0\n",
      "1000\n",
      "len whole= 1000\n",
      "9000\n",
      "len whole= 10000\n",
      "10000\n",
      "2\n",
      "len whole= 0\n",
      "1000\n",
      "len whole= 1000\n",
      "9000\n",
      "len whole= 10000\n",
      "10000\n",
      "3\n",
      "len whole= 0\n",
      "1000\n",
      "len whole= 1000\n",
      "9000\n",
      "len whole= 10000\n",
      "10000\n",
      "4\n",
      "len whole= 0\n",
      "1000\n",
      "len whole= 1000\n",
      "9000\n",
      "len whole= 10000\n",
      "10000\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:14:54.443598Z",
     "start_time": "2025-03-30T21:14:54.405505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for j in [0,1,2,3,4]:\n",
    "    print(j)\n",
    "    indexes = indexes_list[j]\n",
    "    whole = set()\n",
    "    print(\"len whole=\", len(whole))\n",
    "    for i in [indexes[\"retrain_index\"], indexes[\"forget_index\"]]:\n",
    "        print(len(i))\n",
    "        whole = whole.union(set(i))\n",
    "        print(\"len whole=\", len(whole))\n",
    "\n",
    "    print(len(whole))"
   ],
   "id": "dfaabca1df22f3cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "len whole= 0\n",
      "8550\n",
      "len whole= 8550\n",
      "450\n",
      "len whole= 9000\n",
      "9000\n",
      "1\n",
      "len whole= 0\n",
      "8550\n",
      "len whole= 8550\n",
      "450\n",
      "len whole= 9000\n",
      "9000\n",
      "2\n",
      "len whole= 0\n",
      "8550\n",
      "len whole= 8550\n",
      "450\n",
      "len whole= 9000\n",
      "9000\n",
      "3\n",
      "len whole= 0\n",
      "8550\n",
      "len whole= 8550\n",
      "450\n",
      "len whole= 9000\n",
      "9000\n",
      "4\n",
      "len whole= 0\n",
      "8550\n",
      "len whole= 8550\n",
      "450\n",
      "len whole= 9000\n",
      "9000\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T21:17:48.168685Z",
     "start_time": "2025-03-30T21:17:48.111238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for j in [\"full_training_index\", \"test_index\"]:\n",
    "    print(j)\n",
    "    whole = set()\n",
    "    print(\"len whole=\", len(whole))\n",
    "    for i in [0,1,2,3,4]:\n",
    "        print(len(indexes_list[i][j]))\n",
    "        whole = whole.union(set(indexes_list[i][j]))\n",
    "\n",
    "    print(len(whole))"
   ],
   "id": "6e4335f504f38743",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_training_index\n",
      "len whole= 0\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "50000\n",
      "test_index\n",
      "len whole= 0\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "2000\n",
      "10000\n"
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
