{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-29T12:45:47.892898Z",
     "start_time": "2025-03-29T12:45:47.887903Z"
    }
   },
   "source": [
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Define transforms once outside the function\n",
    "pytorch_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "def apply_transforms(batch):\n",
    "    batch[\"img\"] = [pytorch_transforms(img) for img in batch[\"img\"]]\n",
    "    return batch\n",
    "\n",
    "\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T12:55:34.031951Z",
     "start_time": "2025-03-29T12:55:32.490650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Define the dataset root directory\n",
    "root = '/opt/codes/Fed-SCRUB/data'\n",
    "\n",
    "# Define transformations (optional)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset = datasets.CIFAR10(root=root, train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "# Combine both datasets\n",
    "combined_dataset = ConcatDataset([train_dataset, test_dataset])\n",
    "\n",
    "# Check the total length of the combined dataset\n",
    "print(f\"Total samples: {len(combined_dataset)}\")"
   ],
   "id": "8faf86bbd75418cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 60000\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from torch.utils.data import ConcatDataset",
   "id": "b872f686023888f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T12:56:08.854623Z",
     "start_time": "2025-03-29T12:56:08.841605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def configure_balanced_partition(root: str, dataset_name: str, partition_id: int, num_partitions: int, seed: int,\n",
    "                                 shuffle: bool):\n",
    "    \"\"\"\n",
    "    Load a dataset and partition it so that each partition gets the same number of samples per label.\n",
    "\n",
    "    Args:\n",
    "        root (str): Path to dataset storage.\n",
    "        dataset_name (str): Name of the dataset (currently only \"CIFAR10\" is supported).\n",
    "        partition_id (int): The partition index (0 to num_partitions - 1).\n",
    "        num_partitions (int): Total number of partitions.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        shuffle (bool): Whether to shuffle indices within each label partition.\n",
    "\n",
    "    Returns:\n",
    "        Subset: A PyTorch Subset containing the balanced partition of data.\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    if dataset_name.lower() == \"cifar10\":\n",
    "        # Load both training and testing datasets with the same transform\n",
    "\n",
    "        # Load training and test datasets\n",
    "        train_dataset = datasets.CIFAR10(root=root, train=True, download=True, transform=transform)\n",
    "        test_dataset = datasets.CIFAR10(root=root, train=False, download=True, transform=transform)\n",
    "\n",
    "# Combine both datasets\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset\")\n",
    "    print(len(train_dataset))\n",
    "\n",
    "    # Validate partition_id\n",
    "    if not (0 <= partition_id < num_partitions):\n",
    "        raise ValueError(f\"partition_id must be between 0 and {num_partitions - 1}\")\n",
    "\n",
    "    # Group indices by label (CIFAR10 labels are in dataset.targets)\n",
    "    label_to_indices = {}\n",
    "    for idx, label in enumerate(test_dataset.targets):\n",
    "        label_to_indices.setdefault(label, []).append(idx)\n",
    "\n",
    "    partition_indices = []\n",
    "\n",
    "    # For each label, shuffle and evenly split indices\n",
    "    for label, indices in label_to_indices.items():\n",
    "        # Set the seed for reproducibility, then shuffle the indices for this label.\n",
    "        random.seed(seed)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # Determine the size of each partition for this label.\n",
    "        base_size = len(indices) // num_partitions\n",
    "        extra = len(indices) % num_partitions  # Some partitions may get one extra sample\n",
    "\n",
    "        # Compute starting index for the current partition.\n",
    "        # Partitions with an index less than `extra` receive one extra sample.\n",
    "        start_idx = sum(base_size + 1 if i < extra else base_size for i in range(partition_id))\n",
    "        part_size = base_size + 1 if partition_id < extra else base_size\n",
    "        end_idx = start_idx + part_size\n",
    "\n",
    "        label_partition = indices[start_idx:end_idx]\n",
    "\n",
    "        # Optionally shuffle the slice from this label if desired.\n",
    "        if shuffle:\n",
    "            random.shuffle(label_partition)\n",
    "\n",
    "        partition_indices.extend(label_partition)\n",
    "\n",
    "    # Optionally shuffle the combined partition indices.\n",
    "    if shuffle:\n",
    "        random.shuffle(partition_indices)\n",
    "\n",
    "    return partition_indices"
   ],
   "id": "d02c888418a9eaab",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T12:56:13.263720Z",
     "start_time": "2025-03-29T12:56:11.974247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "incides = []\n",
    "for i in [0,1,2,3]:\n",
    "    incides.append(configure_balanced_partition(\"/opt/codes/Fed-SCRUB/data\", \"cifar10\", i, 4, 42, True))"
   ],
   "id": "f95d905de044c4df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConcatDataset' object has no attribute 'targets'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m incides = []\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m [\u001B[32m0\u001B[39m,\u001B[32m1\u001B[39m,\u001B[32m2\u001B[39m,\u001B[32m3\u001B[39m]:\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     incides.append(\u001B[43mconfigure_balanced_partition\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/opt/codes/Fed-SCRUB/data\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcifar10\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m42\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36mconfigure_balanced_partition\u001B[39m\u001B[34m(root, dataset_name, partition_id, num_partitions, seed, shuffle)\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# Group indices by label (CIFAR10 labels are in dataset.targets)\u001B[39;00m\n\u001B[32m     36\u001B[39m label_to_indices = {}\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m idx, label \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtargets\u001B[49m):\n\u001B[32m     38\u001B[39m     label_to_indices.setdefault(label, []).append(idx)\n\u001B[32m     40\u001B[39m partition_indices = []\n",
      "\u001B[31mAttributeError\u001B[39m: 'ConcatDataset' object has no attribute 'targets'"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-29T12:49:59.084453Z",
     "start_time": "2025-03-29T12:49:59.074394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whole = set()\n",
    "for i in incides:\n",
    "    print(len(i))\n",
    "    whole = whole.union(set(i))\n",
    "\n",
    "print(len(whole))\n"
   ],
   "id": "cbb841d40cd7891f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "2500\n",
      "2500\n",
      "2500\n",
      "10000\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
